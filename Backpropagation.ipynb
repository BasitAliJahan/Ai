{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tc0zdcY10ut",
        "outputId": "5da58db4-a3d5-4451-e063-2370c8ab0aab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nbformat': 4,\n",
              " 'nbformat_minor': 0,\n",
              " 'metadata': {'colab': {'provenance': []},\n",
              "  'kernelspec': {'name': 'python3', 'display_name': 'Python 3'},\n",
              "  'language_info': {'name': 'python'}},\n",
              " 'cells': [{'cell_type': 'code',\n",
              "   'source': ['\"\"\"\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    'Network structure:\\n',\n",
              "    ' - Inputs: x1, x2\\n',\n",
              "    ' - Hidden layer: H1, H2 (sigmoid activation)\\n',\n",
              "    ' - Outputs: y1, y2 (sigmoid activation)\\n',\n",
              "    ' - Loss: Sum of squared errors (E_total = 1/2*(T1-y1)^2 + 1/2*(T2-y2)^2)\\n',\n",
              "    '\\n',\n",
              "    '\\n',\n",
              "    '\"\"\"\\n',\n",
              "    '\\n',\n",
              "    'import numpy as np\\n',\n",
              "    '\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '# 1) Setup: inputs, initial params\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '\\n',\n",
              "    '# Inputs (single training example)\\n',\n",
              "    'x1 = 0.07\\n',\n",
              "    'x2 = 0.06\\n',\n",
              "    '\\n',\n",
              "    '# Initial weights (input -> hidden)\\n',\n",
              "    'w1, w2 = 0.10, 0.20  # weights to H1 from x1, x2\\n',\n",
              "    'w3, w4 = 0.20, 0.30  # weights to H2 from x1, x2\\n',\n",
              "    '\\n',\n",
              "    '# Initial weights (hidden -> output)\\n',\n",
              "    'w5, w6 = 0.43, 0.47  # weights to y1 from H1, H2\\n',\n",
              "    'w7, w8 = 0.52, 0.59  # weights to y2 from H1, H2\\n',\n",
              "    '\\n',\n",
              "    '# Biases\\n',\n",
              "    'b1 = 0.31  # bias for hidden neurons H1, H2 (same in example)\\n',\n",
              "    'b2 = 0.28  # bias for output neurons y1, y2 (same in example)\\n',\n",
              "    '\\n',\n",
              "    '# Targets (desired outputs)\\n',\n",
              "    'T1 = 0.66\\n',\n",
              "    'T2 = 0.85\\n',\n",
              "    '\\n',\n",
              "    '# Learning rate\\n',\n",
              "    'lr = 0.5\\n',\n",
              "    '\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '# 2) Activation functions\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '\\n',\n",
              "    'def sigmoid(z):\\n',\n",
              "    '    \"\"\"Sigmoid activation.\"\"\"\\n',\n",
              "    '    return 1.0 / (1.0 + np.exp(-z))\\n',\n",
              "    '\\n',\n",
              "    'def sigmoid_derivative_from_activation(a):\\n',\n",
              "    '    \"\"\"Derivative of sigmoid given activation a = sigmoid(z): a*(1-a).\"\"\"\\n',\n",
              "    '    return a * (1.0 - a)\\n',\n",
              "    '\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '# 3) Helper: forward pass\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '\\n',\n",
              "    'def forward_pass(x1, x2, w1, w2, w3, w4, w5, w6, w7, w8, b1, b2):\\n',\n",
              "    '    \"\"\"\\n',\n",
              "    '    Compute net inputs and activations for hidden and output layers.\\n',\n",
              "    '    Returns a dict with nets and activations for printing/teaching.\\n',\n",
              "    '    \"\"\"\\n',\n",
              "    '    # Hidden layer linear combinations (net inputs)\\n',\n",
              "    '    H1_net = x1 * w1 + x2 * w2 + b1\\n',\n",
              "    '    H2_net = x1 * w3 + x2 * w4 + b1\\n',\n",
              "    '\\n',\n",
              "    '    # Hidden activations (sigmoid)\\n',\n",
              "    '    H1 = sigmoid(H1_net)\\n',\n",
              "    '    H2 = sigmoid(H2_net)\\n',\n",
              "    '\\n',\n",
              "    '    # Output layer linear combinations\\n',\n",
              "    '    y1_net = H1 * w5 + H2 * w6 + b2\\n',\n",
              "    '    y2_net = H1 * w7 + H2 * w8 + b2\\n',\n",
              "    '\\n',\n",
              "    '    # Output activations (sigmoid)\\n',\n",
              "    '    y1 = sigmoid(y1_net)\\n',\n",
              "    '    y2 = sigmoid(y2_net)\\n',\n",
              "    '\\n',\n",
              "    '    return {\\n',\n",
              "    '        \"H1_net\": H1_net, \"H2_net\": H2_net,\\n',\n",
              "    '        \"H1\": H1, \"H2\": H2,\\n',\n",
              "    '        \"y1_net\": y1_net, \"y2_net\": y2_net,\\n',\n",
              "    '        \"y1\": y1, \"y2\": y2\\n',\n",
              "    '    }\\n',\n",
              "    '\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '# 4) Forward before update (print step-by-step)\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '\\n',\n",
              "    'print(\"\\\\n=== FORWARD PASS (before weight update) ===\")\\n',\n",
              "    'out = forward_pass(x1, x2, w1, w2, w3, w4, w5, w6, w7, w8, b1, b2)\\n',\n",
              "    '\\n',\n",
              "    'print(f\"H1_net = {out[\\'H1_net\\']:.7f} => H1 = sigmoid(H1_net) = {out[\\'H1\\']:.9f}\")\\n',\n",
              "    'print(f\"H2_net = {out[\\'H2_net\\']:.7f} => H2 = sigmoid(H2_net) = {out[\\'H2\\']:.9f}\")\\n',\n",
              "    'print(f\"y1_net = {out[\\'y1_net\\']:.9f} => y1 = sigmoid(y1_net) = {out[\\'y1\\']:.9f}\")\\n',\n",
              "    'print(f\"y2_net = {out[\\'y2_net\\']:.9f} => y2 = sigmoid(y2_net) = {out[\\'y2\\']:.9f}\")\\n',\n",
              "    '\\n',\n",
              "    '# Compute per-output squared errors and total error\\n',\n",
              "    \"E1 = 0.5 * (T1 - out['y1'])**2\\n\",\n",
              "    \"E2 = 0.5 * (T2 - out['y2'])**2\\n\",\n",
              "    'E_total = E1 + E2\\n',\n",
              "    'print(f\"\\\\nE1 = 0.5*(T1 - y1)^2 = {E1:.9f}\")\\n',\n",
              "    'print(f\"E2 = 0.5*(T2 - y2)^2 = {E2:.9f}\")\\n',\n",
              "    'print(f\"Total error E_total = E1 + E2 = {E_total:.9f}\")\\n',\n",
              "    '\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '# 5) BACKPROPAGATION — output layer\\n',\n",
              "    '#    compute deltas and gradients for w5..w8\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '\\n',\n",
              "    'print(\"\\\\n=== BACKPROP: output layer ===\")\\n',\n",
              "    '\\n',\n",
              "    '# For each output neuron i: delta_i = dE/dy_i * dy_i/dnet_i\\n',\n",
              "    '# where dE/dy_i = -(T_i - y_i) for E = 1/2*(T-y)^2\\n',\n",
              "    \"dE_dy1 = -(T1 - out['y1'])\\n\",\n",
              "    \"dy1_dnet = sigmoid_derivative_from_activation(out['y1'])\\n\",\n",
              "    'delta1 = dE_dy1 * dy1_dnet  # scalar\\n',\n",
              "    '\\n',\n",
              "    \"dE_dy2 = -(T2 - out['y2'])\\n\",\n",
              "    \"dy2_dnet = sigmoid_derivative_from_activation(out['y2'])\\n\",\n",
              "    'delta2 = dE_dy2 * dy2_dnet  # scalar\\n',\n",
              "    '\\n',\n",
              "    'print(f\"dE/dy1 = {dE_dy1:.9f}, dy1/dnet = {dy1_dnet:.9f}, => delta1 = {delta1:.9f}\")\\n',\n",
              "    'print(f\"dE/dy2 = {dE_dy2:.9f}, dy2/dnet = {dy2_dnet:.9f}, => delta2 = {delta2:.9f}\")\\n',\n",
              "    '\\n',\n",
              "    '# Gradients for weights from hidden -> outputs:\\n',\n",
              "    '# dw5 = dE/dw5 = delta1 * H1\\n',\n",
              "    '# dw6 = dE/dw6 = delta1 * H2\\n',\n",
              "    '# dw7 = dE/dw7 = delta2 * H1\\n',\n",
              "    '# dw8 = dE/dw8 = delta2 * H2\\n',\n",
              "    \"dw5 = delta1 * out['H1']\\n\",\n",
              "    \"dw6 = delta1 * out['H2']\\n\",\n",
              "    \"dw7 = delta2 * out['H1']\\n\",\n",
              "    \"dw8 = delta2 * out['H2']\\n\",\n",
              "    '\\n',\n",
              "    'print(\"\\\\nGradients for hidden->output weights:\")\\n',\n",
              "    'print(f\"dw5 (for w5) = delta1 * H1 = {dw5:.9f}\")\\n',\n",
              "    'print(f\"dw6 (for w6) = delta1 * H2 = {dw6:.9f}\")\\n',\n",
              "    'print(f\"dw7 (for w7) = delta2 * H1 = {dw7:.9f}\")\\n',\n",
              "    'print(f\"dw8 (for w8) = delta2 * H2 = {dw8:.9f}\")\\n',\n",
              "    '\\n',\n",
              "    '# Update output weights (gradient descent): w <- w - lr * dw\\n',\n",
              "    'w5_new = w5 - lr * dw5\\n',\n",
              "    'w6_new = w6 - lr * dw6\\n',\n",
              "    'w7_new = w7 - lr * dw7\\n',\n",
              "    'w8_new = w8 - lr * dw8\\n',\n",
              "    '\\n',\n",
              "    'print(\"\\\\nUpdated hidden->output weights (one step):\")\\n',\n",
              "    'print(f\"w5 -> {w5_new:.9f}\")\\n',\n",
              "    'print(f\"w6 -> {w6_new:.9f}\")\\n',\n",
              "    'print(f\"w7 -> {w7_new:.9f}\")\\n',\n",
              "    'print(f\"w8 -> {w8_new:.9f}\")\\n',\n",
              "    '\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '# 6) BACKPROPAGATION — hidden layer\\n',\n",
              "    '#    compute deltas for H1, H2 and gradients for w1..w4\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '\\n',\n",
              "    'print(\"\\\\n=== BACKPROP: hidden layer ===\")\\n',\n",
              "    '\\n',\n",
              "    '# Error contribution from both output neurons flows back to each hidden neuron:\\n',\n",
              "    \"# delta_H1 = (delta1*w5 + delta2*w7) * sigmoid'(H1_net)\\n\",\n",
              "    \"# delta_H2 = (delta1*w6 + delta2*w8) * sigmoid'(H2_net)\\n\",\n",
              "    '# Note: use the original w5,w6,w7,w8 (the ones that were used in the forward pass)\\n',\n",
              "    \"delta_H1 = (delta1 * w5 + delta2 * w7) * sigmoid_derivative_from_activation(out['H1'])\\n\",\n",
              "    \"delta_H2 = (delta1 * w6 + delta2 * w8) * sigmoid_derivative_from_activation(out['H2'])\\n\",\n",
              "    '\\n',\n",
              "    'print(f\"delta_H1 = (delta1*w5 + delta2*w7) * sigmoid\\'(H1) = {delta_H1:.12f}\")\\n',\n",
              "    'print(f\"delta_H2 = (delta1*w6 + delta2*w8) * sigmoid\\'(H2) = {delta_H2:.12f}\")\\n',\n",
              "    '\\n',\n",
              "    '# Gradients for input->hidden weights:\\n',\n",
              "    '# dw1 = delta_H1 * x1, dw2 = delta_H1 * x2\\n',\n",
              "    '# dw3 = delta_H2 * x1, dw4 = delta_H2 * x2\\n',\n",
              "    'dw1 = delta_H1 * x1\\n',\n",
              "    'dw2 = delta_H1 * x2\\n',\n",
              "    'dw3 = delta_H2 * x1\\n',\n",
              "    'dw4 = delta_H2 * x2\\n',\n",
              "    '\\n',\n",
              "    'print(\"\\\\nGradients for input->hidden weights:\")\\n',\n",
              "    'print(f\"dw1 (for w1) = delta_H1 * x1 = {dw1:.12f}\")\\n',\n",
              "    'print(f\"dw2 (for w2) = delta_H1 * x2 = {dw2:.12f}\")\\n',\n",
              "    'print(f\"dw3 (for w3) = delta_H2 * x1 = {dw3:.12f}\")\\n',\n",
              "    'print(f\"dw4 (for w4) = delta_H2 * x2 = {dw4:.12f}\")\\n',\n",
              "    '\\n',\n",
              "    '# Update hidden weights\\n',\n",
              "    'w1_new = w1 - lr * dw1\\n',\n",
              "    'w2_new = w2 - lr * dw2\\n',\n",
              "    'w3_new = w3 - lr * dw3\\n',\n",
              "    'w4_new = w4 - lr * dw4\\n',\n",
              "    '\\n',\n",
              "    'print(\"\\\\nUpdated input->hidden weights (one step):\")\\n',\n",
              "    'print(f\"w1 -> {w1_new:.9f}\")\\n',\n",
              "    'print(f\"w2 -> {w2_new:.9f}\")\\n',\n",
              "    'print(f\"w3 -> {w3_new:.9f}\")\\n',\n",
              "    'print(f\"w4 -> {w4_new:.9f}\")\\n',\n",
              "    '\\n',\n",
              "    '# For completeness, update biases too (if you want)\\n',\n",
              "    '# bias at output b2 <- b2 - lr * delta (sum of deltas for outputs)\\n',\n",
              "    '# bias at hidden b1 <- b1 - lr * delta_H (sum of deltas for hidden neurons)\\n',\n",
              "    'b2_new = b2 - lr * (delta1 + delta2)  # update using both output deltas\\n',\n",
              "    'b1_new = b1 - lr * (delta_H1 + delta_H2)  # update using hidden deltas\\n',\n",
              "    '\\n',\n",
              "    'print(f\"\\\\nUpdated biases:\")\\n',\n",
              "    'print(f\"b1 -> {b1_new:.9f}\")\\n',\n",
              "    'print(f\"b2 -> {b2_new:.9f}\")\\n',\n",
              "    '\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '# 7) Forward pass after the update (to show error decreased)\\n',\n",
              "    '# -------------------------------\\n',\n",
              "    '\\n',\n",
              "    'print(\"\\\\n=== FORWARD PASS (after weight update) ===\")\\n',\n",
              "    'out_after = forward_pass(x1, x2,\\n',\n",
              "    '                         w1_new, w2_new, w3_new, w4_new,\\n',\n",
              "    '                         w5_new, w6_new, w7_new, w8_new,\\n',\n",
              "    '                         b1_new, b2_new)\\n',\n",
              "    '\\n',\n",
              "    'print(f\"H1 (after) = {out_after[\\'H1\\']:.9f}\")\\n',\n",
              "    'print(f\"H2 (after) = {out_after[\\'H2\\']:.9f}\")\\n',\n",
              "    'print(f\"y1 (after) = {out_after[\\'y1\\']:.9f}\")\\n',\n",
              "    'print(f\"y2 (after) = {out_after[\\'y2\\']:.9f}\")\\n',\n",
              "    '\\n',\n",
              "    \"E1_after = 0.5 * (T1 - out_after['y1'])**2\\n\",\n",
              "    \"E2_after = 0.5 * (T2 - out_after['y2'])**2\\n\",\n",
              "    'E_total_after = E1_after + E2_after\\n',\n",
              "    '\\n',\n",
              "    'print(f\"\\\\nE_total (before) = {E_total:.9f}\")\\n',\n",
              "    'print(f\"E_total (after)  = {E_total_after:.9f}\")\\n',\n",
              "    'print(\"\\\\n(You should see the total error decreased after one backprop step.)\")\\n'],\n",
              "   'metadata': {'colab': {'base_uri': 'https://localhost:8080/'},\n",
              "    'id': 'zPLlQxf5EhSg',\n",
              "    'outputId': 'd50e79a4-0726-4fe9-bf2a-f66553dba951'},\n",
              "   'execution_count': 14,\n",
              "   'outputs': [{'output_type': 'stream',\n",
              "     'name': 'stdout',\n",
              "     'text': ['\\n',\n",
              "      '=== FORWARD PASS (before weight update) ===\\n',\n",
              "      'H1_net = 0.3775000 => H1 = sigmoid(H1_net) = 0.593269992\\n',\n",
              "      'H2_net = 0.3925000 => H2 = sigmoid(H2_net) = 0.596884378\\n',\n",
              "      'y1_net = 1.105905967 => y1 = sigmoid(y1_net) = 0.751365070\\n',\n",
              "      'y2_net = 1.224921404 => y2 = sigmoid(y2_net) = 0.772928465\\n',\n",
              "      '\\n',\n",
              "      'E1 = 0.5*(T1 - y1)^2 = 0.274811083\\n',\n",
              "      'E2 = 0.5*(T2 - y2)^2 = 0.023560026\\n',\n",
              "      'Total error E_total = E1 + E2 = 0.298371109\\n',\n",
              "      '\\n',\n",
              "      '=== BACKPROP: output layer ===\\n',\n",
              "      'dE/dy1 = 0.741365070, dy1/dnet = 0.186815602, => delta1 = 0.138498562\\n',\n",
              "      'dE/dy2 = -0.217071535, dy2/dnet = 0.175510053, => delta2 = -0.038098237\\n',\n",
              "      '\\n',\n",
              "      'Gradients for hidden->output weights:\\n',\n",
              "      'dw5 (for w5) = delta1 * H1 = 0.082167041\\n',\n",
              "      'dw6 (for w6) = delta1 * H2 = 0.082667628\\n',\n",
              "      'dw7 (for w7) = delta2 * H1 = -0.022602540\\n',\n",
              "      'dw8 (for w8) = delta2 * H2 = -0.022740242\\n',\n",
              "      '\\n',\n",
              "      'Updated hidden->output weights (one step):\\n',\n",
              "      'w5 -> 0.358916480\\n',\n",
              "      'w6 -> 0.408666186\\n',\n",
              "      'w7 -> 0.511301270\\n',\n",
              "      'w8 -> 0.561370121\\n',\n",
              "      '\\n',\n",
              "      '=== BACKPROP: hidden layer ===\\n',\n",
              "      \"delta_H1 = (delta1*w5 + delta2*w7) * sigmoid'(H1) = 0.008771354689\\n\",\n",
              "      \"delta_H2 = (delta1*w6 + delta2*w8) * sigmoid'(H2) = 0.009954254705\\n\",\n",
              "      '\\n',\n",
              "      'Gradients for input->hidden weights:\\n',\n",
              "      'dw1 (for w1) = delta_H1 * x1 = 0.000438567734\\n',\n",
              "      'dw2 (for w2) = delta_H1 * x2 = 0.000877135469\\n',\n",
              "      'dw3 (for w3) = delta_H2 * x1 = 0.000497712735\\n',\n",
              "      'dw4 (for w4) = delta_H2 * x2 = 0.000995425471\\n',\n",
              "      '\\n',\n",
              "      'Updated input->hidden weights (one step):\\n',\n",
              "      'w1 -> 0.149780716\\n',\n",
              "      'w2 -> 0.199561432\\n',\n",
              "      'w3 -> 0.249751144\\n',\n",
              "      'w4 -> 0.299502287\\n',\n",
              "      '\\n',\n",
              "      'Updated biases:\\n',\n",
              "      'b1 -> 0.340637195\\n',\n",
              "      'b2 -> 0.549799837\\n',\n",
              "      '\\n',\n",
              "      '=== FORWARD PASS (after weight update) ===\\n',\n",
              "      'H1 (after) = 0.590995531\\n',\n",
              "      'H2 (after) = 0.594614536\\n',\n",
              "      'y1 (after) = 0.732024167\\n',\n",
              "      'y2 (after) = 0.765984653\\n',\n",
              "      '\\n',\n",
              "      'E_total (before) = 0.298371109\\n',\n",
              "      'E_total (after)  = 0.285750887\\n',\n",
              "      '\\n',\n",
              "      '(You should see the total error decreased after one backprop step.)\\n']}]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "{\n",
        "  \"nbformat\": 4,\n",
        "  \"nbformat_minor\": 0,\n",
        "  \"metadata\": {\n",
        "    \"colab\": {\n",
        "      \"provenance\": []\n",
        "    },\n",
        "    \"kernelspec\": {\n",
        "      \"name\": \"python3\",\n",
        "      \"display_name\": \"Python 3\"\n",
        "    },\n",
        "    \"language_info\": {\n",
        "      \"name\": \"python\"\n",
        "    }\n",
        "  },\n",
        "  \"cells\": [\n",
        "    {\n",
        "      \"cell_type\": \"code\",\n",
        "      \"source\": [\n",
        "        \"\\\"\\\"\\\"\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"Network structure:\\n\",\n",
        "        \" - Inputs: x1, x2\\n\",\n",
        "        \" - Hidden layer: H1, H2 (sigmoid activation)\\n\",\n",
        "        \" - Outputs: y1, y2 (sigmoid activation)\\n\",\n",
        "        \" - Loss: Sum of squared errors (E_total = 1/2*(T1-y1)^2 + 1/2*(T2-y2)^2)\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\n\",\n",
        "        \"\\\"\\\"\\\"\\n\",\n",
        "        \"\\n\",\n",
        "        \"import numpy as np\\n\",\n",
        "        \"\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"# 1) Setup: inputs, initial params\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Inputs (single training example)\\n\",\n",
        "        \"x1 = 0.07\\n\",\n",
        "        \"x2 = 0.06\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Initial weights (input -> hidden)\\n\",\n",
        "        \"w1, w2 = 0.10, 0.20  # weights to H1 from x1, x2\\n\",\n",
        "        \"w3, w4 = 0.20, 0.30  # weights to H2 from x1, x2\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Initial weights (hidden -> output)\\n\",\n",
        "        \"w5, w6 = 0.43, 0.47  # weights to y1 from H1, H2\\n\",\n",
        "        \"w7, w8 = 0.52, 0.59  # weights to y2 from H1, H2\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Biases\\n\",\n",
        "        \"b1 = 0.31  # bias for hidden neurons H1, H2 (same in example)\\n\",\n",
        "        \"b2 = 0.28  # bias for output neurons y1, y2 (same in example)\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Targets (desired outputs)\\n\",\n",
        "        \"T1 = 0.66\\n\",\n",
        "        \"T2 = 0.85\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Learning rate\\n\",\n",
        "        \"lr = 0.5\\n\",\n",
        "        \"\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"# 2) Activation functions\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"\\n\",\n",
        "        \"def sigmoid(z):\\n\",\n",
        "        \"    \\\"\\\"\\\"Sigmoid activation.\\\"\\\"\\\"\\n\",\n",
        "        \"    return 1.0 / (1.0 + np.exp(-z))\\n\",\n",
        "        \"\\n\",\n",
        "        \"def sigmoid_derivative_from_activation(a):\\n\",\n",
        "        \"    \\\"\\\"\\\"Derivative of sigmoid given activation a = sigmoid(z): a*(1-a).\\\"\\\"\\\"\\n\",\n",
        "        \"    return a * (1.0 - a)\\n\",\n",
        "        \"\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"# 3) Helper: forward pass\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"\\n\",\n",
        "        \"def forward_pass(x1, x2, w1, w2, w3, w4, w5, w6, w7, w8, b1, b2):\\n\",\n",
        "        \"    \\\"\\\"\\\"\\n\",\n",
        "        \"    Compute net inputs and activations for hidden and output layers.\\n\",\n",
        "        \"    Returns a dict with nets and activations for printing/teaching.\\n\",\n",
        "        \"    \\\"\\\"\\\"\\n\",\n",
        "        \"    # Hidden layer linear combinations (net inputs)\\n\",\n",
        "        \"    H1_net = x1 * w1 + x2 * w2 + b1\\n\",\n",
        "        \"    H2_net = x1 * w3 + x2 * w4 + b1\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Hidden activations (sigmoid)\\n\",\n",
        "        \"    H1 = sigmoid(H1_net)\\n\",\n",
        "        \"    H2 = sigmoid(H2_net)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Output layer linear combinations\\n\",\n",
        "        \"    y1_net = H1 * w5 + H2 * w6 + b2\\n\",\n",
        "        \"    y2_net = H1 * w7 + H2 * w8 + b2\\n\",\n",
        "        \"\\n\",\n",
        "        \"    # Output activations (sigmoid)\\n\",\n",
        "        \"    y1 = sigmoid(y1_net)\\n\",\n",
        "        \"    y2 = sigmoid(y2_net)\\n\",\n",
        "        \"\\n\",\n",
        "        \"    return {\\n\",\n",
        "        \"        \\\"H1_net\\\": H1_net, \\\"H2_net\\\": H2_net,\\n\",\n",
        "        \"        \\\"H1\\\": H1, \\\"H2\\\": H2,\\n\",\n",
        "        \"        \\\"y1_net\\\": y1_net, \\\"y2_net\\\": y2_net,\\n\",\n",
        "        \"        \\\"y1\\\": y1, \\\"y2\\\": y2\\n\",\n",
        "        \"    }\\n\",\n",
        "        \"\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"# 4) Forward before update (print step-by-step)\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\n=== FORWARD PASS (before weight update) ===\\\")\\n\",\n",
        "        \"out = forward_pass(x1, x2, w1, w2, w3, w4, w5, w6, w7, w8, b1, b2)\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(f\\\"H1_net = {out['H1_net']:.7f} => H1 = sigmoid(H1_net) = {out['H1']:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"H2_net = {out['H2_net']:.7f} => H2 = sigmoid(H2_net) = {out['H2']:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"y1_net = {out['y1_net']:.9f} => y1 = sigmoid(y1_net) = {out['y1']:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"y2_net = {out['y2_net']:.9f} => y2 = sigmoid(y2_net) = {out['y2']:.9f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Compute per-output squared errors and total error\\n\",\n",
        "        \"E1 = 0.5 * (T1 - out['y1'])**2\\n\",\n",
        "        \"E2 = 0.5 * (T2 - out['y2'])**2\\n\",\n",
        "        \"E_total = E1 + E2\\n\",\n",
        "        \"print(f\\\"\\\\nE1 = 0.5*(T1 - y1)^2 = {E1:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"E2 = 0.5*(T2 - y2)^2 = {E2:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"Total error E_total = E1 + E2 = {E_total:.9f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"# 5) BACKPROPAGATION — output layer\\n\",\n",
        "        \"#    compute deltas and gradients for w5..w8\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\n=== BACKPROP: output layer ===\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# For each output neuron i: delta_i = dE/dy_i * dy_i/dnet_i\\n\",\n",
        "        \"# where dE/dy_i = -(T_i - y_i) for E = 1/2*(T-y)^2\\n\",\n",
        "        \"dE_dy1 = -(T1 - out['y1'])\\n\",\n",
        "        \"dy1_dnet = sigmoid_derivative_from_activation(out['y1'])\\n\",\n",
        "        \"delta1 = dE_dy1 * dy1_dnet  # scalar\\n\",\n",
        "        \"\\n\",\n",
        "        \"dE_dy2 = -(T2 - out['y2'])\\n\",\n",
        "        \"dy2_dnet = sigmoid_derivative_from_activation(out['y2'])\\n\",\n",
        "        \"delta2 = dE_dy2 * dy2_dnet  # scalar\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(f\\\"dE/dy1 = {dE_dy1:.9f}, dy1/dnet = {dy1_dnet:.9f}, => delta1 = {delta1:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"dE/dy2 = {dE_dy2:.9f}, dy2/dnet = {dy2_dnet:.9f}, => delta2 = {delta2:.9f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Gradients for weights from hidden -> outputs:\\n\",\n",
        "        \"# dw5 = dE/dw5 = delta1 * H1\\n\",\n",
        "        \"# dw6 = dE/dw6 = delta1 * H2\\n\",\n",
        "        \"# dw7 = dE/dw7 = delta2 * H1\\n\",\n",
        "        \"# dw8 = dE/dw8 = delta2 * H2\\n\",\n",
        "        \"dw5 = delta1 * out['H1']\\n\",\n",
        "        \"dw6 = delta1 * out['H2']\\n\",\n",
        "        \"dw7 = delta2 * out['H1']\\n\",\n",
        "        \"dw8 = delta2 * out['H2']\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\nGradients for hidden->output weights:\\\")\\n\",\n",
        "        \"print(f\\\"dw5 (for w5) = delta1 * H1 = {dw5:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"dw6 (for w6) = delta1 * H2 = {dw6:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"dw7 (for w7) = delta2 * H1 = {dw7:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"dw8 (for w8) = delta2 * H2 = {dw8:.9f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Update output weights (gradient descent): w <- w - lr * dw\\n\",\n",
        "        \"w5_new = w5 - lr * dw5\\n\",\n",
        "        \"w6_new = w6 - lr * dw6\\n\",\n",
        "        \"w7_new = w7 - lr * dw7\\n\",\n",
        "        \"w8_new = w8 - lr * dw8\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\nUpdated hidden->output weights (one step):\\\")\\n\",\n",
        "        \"print(f\\\"w5 -> {w5_new:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"w6 -> {w6_new:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"w7 -> {w7_new:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"w8 -> {w8_new:.9f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"# 6) BACKPROPAGATION — hidden layer\\n\",\n",
        "        \"#    compute deltas for H1, H2 and gradients for w1..w4\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\n=== BACKPROP: hidden layer ===\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Error contribution from both output neurons flows back to each hidden neuron:\\n\",\n",
        "        \"# delta_H1 = (delta1*w5 + delta2*w7) * sigmoid'(H1_net)\\n\",\n",
        "        \"# delta_H2 = (delta1*w6 + delta2*w8) * sigmoid'(H2_net)\\n\",\n",
        "        \"# Note: use the original w5,w6,w7,w8 (the ones that were used in the forward pass)\\n\",\n",
        "        \"delta_H1 = (delta1 * w5 + delta2 * w7) * sigmoid_derivative_from_activation(out['H1'])\\n\",\n",
        "        \"delta_H2 = (delta1 * w6 + delta2 * w8) * sigmoid_derivative_from_activation(out['H2'])\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(f\\\"delta_H1 = (delta1*w5 + delta2*w7) * sigmoid'(H1) = {delta_H1:.12f}\\\")\\n\",\n",
        "        \"print(f\\\"delta_H2 = (delta1*w6 + delta2*w8) * sigmoid'(H2) = {delta_H2:.12f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Gradients for input->hidden weights:\\n\",\n",
        "        \"# dw1 = delta_H1 * x1, dw2 = delta_H1 * x2\\n\",\n",
        "        \"# dw3 = delta_H2 * x1, dw4 = delta_H2 * x2\\n\",\n",
        "        \"dw1 = delta_H1 * x1\\n\",\n",
        "        \"dw2 = delta_H1 * x2\\n\",\n",
        "        \"dw3 = delta_H2 * x1\\n\",\n",
        "        \"dw4 = delta_H2 * x2\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\nGradients for input->hidden weights:\\\")\\n\",\n",
        "        \"print(f\\\"dw1 (for w1) = delta_H1 * x1 = {dw1:.12f}\\\")\\n\",\n",
        "        \"print(f\\\"dw2 (for w2) = delta_H1 * x2 = {dw2:.12f}\\\")\\n\",\n",
        "        \"print(f\\\"dw3 (for w3) = delta_H2 * x1 = {dw3:.12f}\\\")\\n\",\n",
        "        \"print(f\\\"dw4 (for w4) = delta_H2 * x2 = {dw4:.12f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# Update hidden weights\\n\",\n",
        "        \"w1_new = w1 - lr * dw1\\n\",\n",
        "        \"w2_new = w2 - lr * dw2\\n\",\n",
        "        \"w3_new = w3 - lr * dw3\\n\",\n",
        "        \"w4_new = w4 - lr * dw4\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\nUpdated input->hidden weights (one step):\\\")\\n\",\n",
        "        \"print(f\\\"w1 -> {w1_new:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"w2 -> {w2_new:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"w3 -> {w3_new:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"w4 -> {w4_new:.9f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# For completeness, update biases too (if you want)\\n\",\n",
        "        \"# bias at output b2 <- b2 - lr * delta (sum of deltas for outputs)\\n\",\n",
        "        \"# bias at hidden b1 <- b1 - lr * delta_H (sum of deltas for hidden neurons)\\n\",\n",
        "        \"b2_new = b2 - lr * (delta1 + delta2)  # update using both output deltas\\n\",\n",
        "        \"b1_new = b1 - lr * (delta_H1 + delta_H2)  # update using hidden deltas\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(f\\\"\\\\nUpdated biases:\\\")\\n\",\n",
        "        \"print(f\\\"b1 -> {b1_new:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"b2 -> {b2_new:.9f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"# 7) Forward pass after the update (to show error decreased)\\n\",\n",
        "        \"# -------------------------------\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(\\\"\\\\n=== FORWARD PASS (after weight update) ===\\\")\\n\",\n",
        "        \"out_after = forward_pass(x1, x2,\\n\",\n",
        "        \"                         w1_new, w2_new, w3_new, w4_new,\\n\",\n",
        "        \"                         w5_new, w6_new, w7_new, w8_new,\\n\",\n",
        "        \"                         b1_new, b2_new)\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(f\\\"H1 (after) = {out_after['H1']:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"H2 (after) = {out_after['H2']:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"y1 (after) = {out_after['y1']:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"y2 (after) = {out_after['y2']:.9f}\\\")\\n\",\n",
        "        \"\\n\",\n",
        "        \"E1_after = 0.5 * (T1 - out_after['y1'])**2\\n\",\n",
        "        \"E2_after = 0.5 * (T2 - out_after['y2'])**2\\n\",\n",
        "        \"E_total_after = E1_after + E2_after\\n\",\n",
        "        \"\\n\",\n",
        "        \"print(f\\\"\\\\nE_total (before) = {E_total:.9f}\\\")\\n\",\n",
        "        \"print(f\\\"E_total (after)  = {E_total_after:.9f}\\\")\\n\",\n",
        "        \"print(\\\"\\\\n(You should see the total error decreased after one backprop step.)\\\")\\n\"\n",
        "      ],\n",
        "      \"metadata\": {\n",
        "        \"colab\": {\n",
        "          \"base_uri\": \"https://localhost:8080/\"\n",
        "        },\n",
        "        \"id\": \"zPLlQxf5EhSg\",\n",
        "        \"outputId\": \"d50e79a4-0726-4fe9-bf2a-f66553dba951\"\n",
        "      },\n",
        "      \"execution_count\": 14,\n",
        "      \"outputs\": [\n",
        "        {\n",
        "          \"output_type\": \"stream\",\n",
        "          \"name\": \"stdout\",\n",
        "          \"text\": [\n",
        "            \"\\n\",\n",
        "            \"=== FORWARD PASS (before weight update) ===\\n\",\n",
        "            \"H1_net = 0.3775000 => H1 = sigmoid(H1_net) = 0.593269992\\n\",\n",
        "            \"H2_net = 0.3925000 => H2 = sigmoid(H2_net) = 0.596884378\\n\",\n",
        "            \"y1_net = 1.105905967 => y1 = sigmoid(y1_net) = 0.751365070\\n\",\n",
        "            \"y2_net = 1.224921404 => y2 = sigmoid(y2_net) = 0.772928465\\n\",\n",
        "            \"\\n\",\n",
        "            \"E1 = 0.5*(T1 - y1)^2 = 0.274811083\\n\",\n",
        "            \"E2 = 0.5*(T2 - y2)^2 = 0.023560026\\n\",\n",
        "            \"Total error E_total = E1 + E2 = 0.298371109\\n\",\n",
        "            \"\\n\",\n",
        "            \"=== BACKPROP: output layer ===\\n\",\n",
        "            \"dE/dy1 = 0.741365070, dy1/dnet = 0.186815602, => delta1 = 0.138498562\\n\",\n",
        "            \"dE/dy2 = -0.217071535, dy2/dnet = 0.175510053, => delta2 = -0.038098237\\n\",\n",
        "            \"\\n\",\n",
        "            \"Gradients for hidden->output weights:\\n\",\n",
        "            \"dw5 (for w5) = delta1 * H1 = 0.082167041\\n\",\n",
        "            \"dw6 (for w6) = delta1 * H2 = 0.082667628\\n\",\n",
        "            \"dw7 (for w7) = delta2 * H1 = -0.022602540\\n\",\n",
        "            \"dw8 (for w8) = delta2 * H2 = -0.022740242\\n\",\n",
        "            \"\\n\",\n",
        "            \"Updated hidden->output weights (one step):\\n\",\n",
        "            \"w5 -> 0.358916480\\n\",\n",
        "            \"w6 -> 0.408666186\\n\",\n",
        "            \"w7 -> 0.511301270\\n\",\n",
        "            \"w8 -> 0.561370121\\n\",\n",
        "            \"\\n\",\n",
        "            \"=== BACKPROP: hidden layer ===\\n\",\n",
        "            \"delta_H1 = (delta1*w5 + delta2*w7) * sigmoid'(H1) = 0.008771354689\\n\",\n",
        "            \"delta_H2 = (delta1*w6 + delta2*w8) * sigmoid'(H2) = 0.009954254705\\n\",\n",
        "            \"\\n\",\n",
        "            \"Gradients for input->hidden weights:\\n\",\n",
        "            \"dw1 (for w1) = delta_H1 * x1 = 0.000438567734\\n\",\n",
        "            \"dw2 (for w2) = delta_H1 * x2 = 0.000877135469\\n\",\n",
        "            \"dw3 (for w3) = delta_H2 * x1 = 0.000497712735\\n\",\n",
        "            \"dw4 (for w4) = delta_H2 * x2 = 0.000995425471\\n\",\n",
        "            \"\\n\",\n",
        "            \"Updated input->hidden weights (one step):\\n\",\n",
        "            \"w1 -> 0.149780716\\n\",\n",
        "            \"w2 -> 0.199561432\\n\",\n",
        "            \"w3 -> 0.249751144\\n\",\n",
        "            \"w4 -> 0.299502287\\n\",\n",
        "            \"\\n\",\n",
        "            \"Updated biases:\\n\",\n",
        "            \"b1 -> 0.340637195\\n\",\n",
        "            \"b2 -> 0.549799837\\n\",\n",
        "            \"\\n\",\n",
        "            \"=== FORWARD PASS (after weight update) ===\\n\",\n",
        "            \"H1 (after) = 0.590995531\\n\",\n",
        "            \"H2 (after) = 0.594614536\\n\",\n",
        "            \"y1 (after) = 0.732024167\\n\",\n",
        "            \"y2 (after) = 0.765984653\\n\",\n",
        "            \"\\n\",\n",
        "            \"E_total (before) = 0.298371109\\n\",\n",
        "            \"E_total (after)  = 0.285750887\\n\",\n",
        "            \"\\n\",\n",
        "            \"(You should see the total error decreased after one backprop step.)\\n\"\n",
        "          ]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}"
      ]
    }
  ]
}